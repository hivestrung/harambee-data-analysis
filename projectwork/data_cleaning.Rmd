---
title: "iX Project - Shiv"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/shiv/Desktop/iX/ix_cape_town_2019_project")
library(tidyverse)
library(lubridate)
library(hablar)
library(caret)

```

Wiki: https://github.com/neilrankinza/ix_cape_town_2019_project/wiki

Build a machine learning model to:
1. Predict who is likely to be in work (in survey 1) so that they can intervene at ‘baseline’
2. Predict who is likely to work for more than 6 months

# Import the data
```{r, echo=FALSE}
data <- read_csv("data/raw/teaching_training_data.csv") %>% select(-X1)
cft <- read.csv("data/raw/teaching_training_data_cft.csv") %>% select(-X)
com <- read.csv("data/raw/teaching_training_data_com.csv") %>% select(-X)
grit <- read.csv("data/raw/teaching_training_data_grit.csv") %>% select(-X)
num <- read.csv("data/raw/teaching_training_data_num.csv") %>% select(-X)
opt <- read.csv("data/raw/teaching_training_data_opt.csv") %>% select(-X)
```

# Understand the data

```{r}
head(data)
head(cft)
head(com)
head(grit)
head(num)
head(opt)
```

```{r}
summary(data)
```
There are a lot of strange occurrences in the data

1. There is a codependency between peoplelive, peoplelive_15plus, numchildren and numearnincome. However, the data does not always seem to match up / make sense. For example, unid 690 is living with more than 15 people, and 8 in the household are earning income, but she did not fill in the number of members above 15 years of age. Meanwhile there is 1 child in the household. So, there are definitely members above 15 years of age, but we can't easily impute the data. 

2. Sometimes, peoplelive is less than numchildren - are these children not living at home? We do not know.
Insight: Define terms used. Also, perform data validation to ensure logical responses (and reject illogical ones) - applies only for digital survey collection.

3. Sometimes, numearnincome indicated they lived alone, but peoplelive is more than 0. Consider unid 624 and 393 where this happened (there are more examples of this.)

4. Does numearnincome include the survey respondent? Even for working individuals, there are cases of the number being 0, which means some people are interpreting it to not include themselves. However, (e.g. unid 6336), numearnincome is 1, while peoplelive (excludes respondent) is 0. So the person earning income is himself. So the data is not consistently collected due to different interpretations.

Insight: Clearly define all terms used in survey questions to avoid ambiguity.

5. There are some people who are not currently working (based on the job_leave_date being before survey date) but working is True.


# Sort the data and rename unid to id, then order it such that id is in the front
```{r}
data = arrange(data, unid)
data = mutate(data, id = unid) %>% select(-unid)
data = data[, c(dim(data)[2], 1:dim(data)[2]-1)]

```
# Split into work and no work groups

```{r}
data.work = data %>% filter(working==T) %>% select(-working)
data.nowork = data %>% filter(working==F)
```

# Cleanup

## Handling NAs in job_start_date and/or job_leave_date

1. If job_start_date is NA, remove row
```{r, echo=FALSE}
data.work = data.work %>% filter(!is.na(job_start_date))
```
At this point, all rows have a valid job_start_date

2. If job_leave_date is NA, we assume he is still working at the date of survey, hence replace job_leave_date with survey_date_month
```{r}
data.work$job_leave_date = as_date(ifelse(is.na(data.work$job_leave_date), data.work$survey_date_month, data.work$job_leave_date))
```

## Parsing peoplelive column
Some of the data entries are "0: I live alone" or "more than 15" etc, so they have to be parsed.

Do data clipping, by assuming "more than 15" is just 15.

```{r}
data.work <- data.work %>%
        mutate(peoplelive = parse_number(as.character(peoplelive))) %>%
        mutate(peoplelive_15plus = parse_number(as.character(peoplelive_15plus))) %>%
        mutate(numearnincome = parse_number(as.character(numearnincome)))
```




# Feature Engineering

1. Based on job_start_date and job_leave_date, create a column daysWorked, which gives the number of days worked
```{r}
data.work$daysWorked = as.Date(as.character(data.work$job_leave_date), format="%Y-%m-%d")-
                  as.Date(as.character(data.work$job_start_date), format="%Y-%m-%d") + 1
```
2. Based on daysWorked, create a column modeThanSixMths indicating whether the person has worked more than 6 months
```{r}
data.work$moreThanSixMths = ifelse(data.work$daysWorked >= 180, T, F)
```

3. Based on date of birth (dob) and survey_date_month, create column age_at_survey (and round to the nearest integer)
```{r}
data.work <- data.work %>%
 mutate(age_at_survey = interval(dob, survey_date_month)/years(1)) %>%
 mutate(age_at_survey = round(age_at_survey))
```

4. Parse the text from the financial situation columns, and create a column fin_situ_change that gives us the expected change in financial situation

```{r}
data.work <- data.work %>%
 mutate(fin_situ_now = parse_number(as.character(financial_situation_now))) %>%
 mutate(fin_situ_future = parse_number(as.character(financial_situation_5years))) %>%
 mutate(fin_situ_change = fin_situ_future - fin_situ_now)
```

Finally, let's remove the redundant data we've already used to engineer features
```{r}
data.work = data.work %>% select(-job_start_date ,-job_leave_date, -dob, -financial_situation_now, -financial_situation_5years)
```

# Working with incomplete data

Check the number of NAs in every column, and calculate percentage of na's in column

```{r}
na_count = sapply(data.work, function(y) sum(length(which(is.na(y)))))
na_count = data.frame(na_count)
dim(na_count)

# create column from index
names <- rownames(na_count)
# nullify index
rownames(na_count) <- NULL
na_count <- cbind(names,na_count)

na_count = mutate(na_count, percentageNA = 100*na_count/dim(data.work)[1])
na_count = arrange(na_count, desc(percentageNA))
na_count
```
As we can see, company_size and monthly_pay are about 90% NAs. However, these are likely very important in determining whether a person is likely to stay at their job (for instance, if you're paid well, you're incentivised to stay on and work hard.) So, we will want to impute the data for this.

First, we'll fill in the rest of the columns, making some assumptions along the way.

Remove peoplelive_15plus because it is 80% incomplete and does not add much because we already have numearnincome to quantify number of independents in the household

```{r}
data.work = select(data.work, -peoplelive_15plus)
```

Assume that:
1. if givemoney_yes, volunteer, leadershiprole, anygrant is NA, it is no
2. if anyhhincome is NA, it is yes (because we are working with data for people who are working)
3. if peoplelive is NA, it is 0
4. 





```{r}
ggplot(data.work, aes(x=numearnincome, y=peoplelive)) + geom_point() + geom_count() + geom_abline(slope=1, intercept=0)+ scale_size_continuous(range = c(1, 8))
```


assume

remove
ppl15plus


impute

province
company_size
monthly_pay


// later we will need to deal with company size and the pay (being strings)







## CLeaning the extra data (from the personality attribute tests)
After some exploration, we realised only opt has NAs

Clean opt because it has NAs
```{r}
opt = na.omit(opt)
```

Grit has multiple readings per individual, visualising this, which plots a graph of number of people against how many grit readings they have

```{r}
grit_analysis  = group_by(grit, unid) %>% count() %>% group_by(n) %>% count()
ggplot(grit_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of grit readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(grit_analysis$n), max(grit_analysis$n), by =1),1))
```


```{r}
opt_analysis  = group_by(opt, unid) %>% count() %>% group_by(n) %>% count()
ggplot(opt_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of opt readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(opt_analysis$n), max(opt_analysis$n), by =1),1)) 
```

```{r}
com_analysis  = group_by(com, unid) %>% count() %>% group_by(n) %>% count()
ggplot(com_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of com readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(com_analysis$n), max(com_analysis$n), by =1),1))
```


```{r}
cft_analysis  = group_by(cft, unid) %>% count() %>% group_by(n) %>% count()
ggplot(cft_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of cft readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(cft_analysis$n), max(cft_analysis$n), by =1),1))
```

```{r}
num_analysis  = group_by(num, unid) %>% count() %>% group_by(n) %>% count()
ggplot(num_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of num readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(num_analysis$n), max(num_analysis$n), by =1),1))
```

We will just take the mean to break ties

```{r}
grit_clean = group_by(grit,unid)  %>% summarise(mean_grit_score=mean(grit_score))
grit_clean$mean_grit_score = round(grit_clean$mean_grit_score)

com_clean = group_by(com,unid)  %>% summarise(mean_com_score=mean(com_score))
com_clean$mean_com_score = round(com_clean$mean_com_score)

opt_clean = group_by(opt,unid)  %>% summarise(mean_opt_score=mean(opt_score))
opt_clean$mean_opt_score = round(opt_clean$mean_opt_score)

cft_clean = group_by(cft,unid)  %>% summarise(mean_cft_score=mean(cft_score))
cft_clean$mean_cft_score = round(cft_clean$mean_cft_score)

num_clean = group_by(num,unid)  %>% summarise(mean_num_score=mean(num_score))
num_clean$mean_num_score = round(num_clean$mean_num_score)
```


merging the datasets
```{r}
data.work.clean.merged = data.work.clean %>% merge(opt_clean, by="unid", all.x=T) %>% merge(num_clean, by="unid", all.x=T) %>% merge(grit_clean,by="unid", all.x=T) %>% merge(cft_clean, by="unid", all.x=T) %>% merge(com_clean, by="unid", all.x=T)

str(data.work.clean.merged)

```


Assuming that if NA, volunteer is no, leadershiprole is no, peoplelive is 0, peoplelive_15plus is 0, numchildren is 0, numearnincome is 0

inspect the data for useless columns (firstly, remove columns with v little data)

```{r}
summary(data.work.clean.merged)
```


clean the data of nas

```{r}
data.work.clean3 = na.omit(data.work.clean2)
```






Finalising the dataset
```{r}
data.work.final = select(data.work.clean.merged, -survey_date_month, -fin_situ_change,     -survey_num, -unid)

```
## viewing the data

```{r}
ggplot(data.work.final, aes(y=moreThanSixMths,x=gender)) + 
```



## performing logistic regression


```{r}
data.work.final$moreThanSixMths =  as.factor(data.work.final$moreThanSixMths)

TRAINCONTROL = trainControl(method = "cv", number = 8, verboseIter = TRUE)

glm_model <- train(moreThanSixMths~., data = data.work.final, method = "rpart", trControl = TRAINCONTROL)
glm_model

```

















