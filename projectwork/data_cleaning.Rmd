---
title: "iX Project - Shiv"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/shiv/Desktop/iX/ix_cape_town_2019_project")
library(tidyverse)
library(lubridate)
library(hablar)
library(caret)

```

Wiki: https://github.com/neilrankinza/ix_cape_town_2019_project/wiki

Build a machine learning model to:
1. Predict who is likely to be in work (in survey 1) so that they can intervene at ‘baseline’
2. Predict who is likely to work for more than 6 months

# Import the data
```{r, echo=FALSE}
setwd("~/Documents/4. GitHub/harambee-data-analysis")
data <- read_csv("data/raw/teaching_training_data.csv") %>% select(-X1)
cft <- read.csv("data/raw/teaching_training_data_cft.csv") %>% select(-X)
com <- read.csv("data/raw/teaching_training_data_com.csv") %>% select(-X)
grit <- read.csv("data/raw/teaching_training_data_grit.csv") %>% select(-X)
num <- read.csv("data/raw/teaching_training_data_num.csv") %>% select(-X)
opt <- read.csv("data/raw/teaching_training_data_opt.csv") %>% select(-X)
```

# Understand the data

```{r}
head(data)
head(cft)
head(com)
head(grit)
head(num)
head(opt)
```

```{r}
summary(data)
```
There are a lot of strange occurrences in the data

1. There is a codependency between peoplelive, peoplelive_15plus, numchildren and numearnincome. However, the data does not always seem to match up / make sense. For example, unid 690 is living with more than 15 people, and 8 in the household are earning income, but she did not fill in the number of members above 15 years of age. Meanwhile there is 1 child in the household. So, there are definitely members above 15 years of age, but we can't easily impute the data. 

2. Sometimes, peoplelive is less than numchildren - are these children not living at home? We do not know.
Insight: Define terms used. Also, perform data validation to ensure logical responses (and reject illogical ones) - applies only for digital survey collection.

3. Sometimes, numearnincome indicated they lived alone, but peoplelive is more than 0. Consider unid 624 and 393 where this happened (there are more examples of this.)

4. Does numearnincome include the survey respondent? Even for working individuals, there are cases of the number being 0, which means some people are interpreting it to not include themselves. However, (e.g. unid 6336), numearnincome is 1, while peoplelive (excludes respondent) is 0. So the person earning income is himself. So the data is not consistently collected due to different interpretations.

Insight: Clearly define all terms used in survey questions to avoid ambiguity.

5. There are some people who are not currently working (based on the job_leave_date being before survey date) but working is True.


```{r}
data = arrange(data, unid)

```
# Split into work and no work groups

```{r}
data.work = data %>% filter(working==T) %>% select(-working)
data.nowork = data %>% filter(working==F)
```

# Cleanup

## Handling NAs in job_start_date and/or job_leave_date

1. If job_start_date is NA, remove row
```{r, echo=FALSE}
data.work = data.work %>% filter(!is.na(job_start_date))
```
At this point, all rows have a valid job_start_date

2. If job_leave_date is NA, we assume he is still working at the date of survey, hence replace job_leave_date with survey_date_month
```{r}
data.work$job_leave_date = as_date(ifelse(is.na(data.work$job_leave_date), data.work$survey_date_month, data.work$job_leave_date))
```

## Parsing peoplelive column
Some of the data entries are "0: I live alone" or "more than 15" etc, so they have to be parsed.

Do data clipping, by assuming "more than 15" is just 15.

```{r}
data.work <- data.work %>%
        mutate(peoplelive = parse_number(as.character(peoplelive))) %>%
        mutate(peoplelive_15plus = parse_number(as.character(peoplelive_15plus))) %>%
        mutate(numearnincome = parse_number(as.character(numearnincome)))
```




# Feature Engineering

1. Based on job_start_date and job_leave_date, create a column daysWorked, which gives the number of days worked
```{r}
data.work$daysWorked = as.Date(as.character(data.work$job_leave_date), format="%Y-%m-%d")-
                  as.Date(as.character(data.work$job_start_date), format="%Y-%m-%d") + 1
```
2. Based on daysWorked, create a column modeThanSixMths indicating whether the person has worked more than 6 months
```{r}
data.work$moreThanSixMths = ifelse(data.work$daysWorked >= 180, T, F)
```

3. Based on date of birth (dob) and survey_date_month, create column age_at_survey (and round to the nearest integer)
```{r}
data.work <- data.work %>%
 mutate(age_at_survey = interval(dob, survey_date_month)/years(1)) %>%
 mutate(age_at_survey = round(age_at_survey))
```

4. Parse the text from the financial situation columns, and create a column fin_situ_change that gives us the expected change in financial situation

```{r}
data.work <- data.work %>%
 mutate(fin_situ_now = parse_number(as.character(financial_situation_now))) %>%
 mutate(fin_situ_future = parse_number(as.character(financial_situation_5years))) %>%
 mutate(fin_situ_change = fin_situ_future - fin_situ_now)
```

Finally, let's remove the redundant data we've already used to engineer features
```{r}
data.work = data.work %>% select(-job_start_date ,-job_leave_date, -dob, -financial_situation_now, -financial_situation_5years)
```

# Working with incomplete data

Check the number of NAs in every column, and calculate percentage of na's in column

```{r}
na_count = sapply(data.work, function(y) sum(length(which(is.na(y)))))
na_count = data.frame(na_count)
dim(na_count)

# create column from index
names <- rownames(na_count)
# nullify index
rownames(na_count) <- NULL
na_count <- cbind(names,na_count)

na_count = mutate(na_count, percentageNA = 100*na_count/dim(data.work)[1])
na_count = arrange(na_count, desc(percentageNA))
na_count
```
As we can see, company_size and monthly_pay are about 90% NAs. However, these are likely very important in determining whether a person is likely to stay at their job (for instance, if you're paid well, you're incentivised to stay on and work hard.) So, we will want to impute the data for this.

First, we'll fill in the rest of the columns, making some assumptions along the way.

## Removal of columns

Remove peoplelive_15plus because it is 80% incomplete and does not add much because we already have numearnincome to quantify number of independents in the household

Also remove anyhhincome because it should all be TRUE given that data.work is a subset of the dataset that is working, so they should receive household income.
Note that the data doesn't all state TRUE so there was probably an issue with data collection here (in terms of clearly stating what household income means)


```{r}
data.work = select(data.work, -peoplelive_15plus, -anyhhincome)
```

## Removal of rows

Remove rows where gender is NA, because there are only 2 such datapoints
Remove rows where age_at_survey is NA because there are only 20 such rows (out of almost 16000)
```{r}
data.work = filter(data.work, !is.na(data.work$gender))
data.work = filter(data.work, !is.na(data.work$age_at_survey))

```

Assume that:
1. if givemoney_yes, volunteer, leadershiprole, anygrant is NA, it is no (FALSE)
2. if peoplelive, numchildren, numearnincome is NA, it is 0

```{r}
data.work$givemoney_yes = ifelse(is.na(data.work$givemoney_yes), FALSE, data.work$givemoney_yes)
data.work$volunteer = ifelse(is.na(data.work$volunteer), FALSE, data.work$volunteer)
data.work$leadershiprole = ifelse(is.na(data.work$leadershiprole), FALSE, data.work$leadershiprole)
data.work$anygrant = ifelse(is.na(data.work$anygrant), FALSE, data.work$anygrant)

data.work$peoplelive = ifelse(is.na(data.work$peoplelive), 0, data.work$peoplelive)
data.work$numchildren = ifelse(is.na(data.work$numchildren), 0, data.work$numchildren)
data.work$numearnincome = ifelse(is.na(data.work$numearnincome), 0, data.work$numearnincome)
```


Visualise the relation between numearnincome and peoplelive
```{r}
ggplot(data.work, aes(x=numearnincome, y=peoplelive)) + geom_point() + geom_count() + geom_abline(slope=1, intercept=-1)+scale_size_continuous(range = c(1, 8))
```
We will assume whether numearnincome include self or not by taking the most commonly used interpretation of the survey respondents.
a = People who definitely assumed that numearnincome does not include self = people indicating 0 numearnincome
b = People who definitely assumed that numearnincome include self = numearnincome exceeds peoplelive by 1 (people who lies on the line y=x-1 shown above)
From graph, a>b.

Hence we should assume that numearnincome does not include self and correct the data by deducting 1 from b's numearnincome.

```{r}
data.work$numearnincome = ifelse(data.work$numearnincome - data.work$peoplelive == 1,data.work$numearnincome-1,data.work$numearnincome)
```

Now we can see that there are no datapoints lying on the line y=x-1
```{r}
ggplot(data.work, aes(x=numearnincome, y=peoplelive)) + geom_point() + geom_count() + geom_abline(slope=1, intercept=-1)+ scale_size_continuous(range = c(1, 8))
```

We then removed all rows of data in which numearnincome > peoplelive (datapoints below the line)

```{r}
data.work = filter(data.work, data.work$numearnincome <= data.work$peoplelive)
```

Now there are no more points on or below the line
```{r}
ggplot(data.work, aes(x=numearnincome, y=peoplelive)) + geom_point() + geom_count() + geom_abline(slope=1, intercept=0)+ scale_size_continuous(range = c(1, 8))
```

Insight: BUT logically speaking since the survey is for people both working and not working, numearnincome was meant to contain self from surveyor's perspective. But this was not made clear to the survey respondents.


Check the number of NAs in every column, and calculate percentage of na's in column

```{r}
na_count = sapply(data.work, function(y) sum(length(which(is.na(y)))))
na_count = data.frame(na_count)
dim(na_count)

# create column from index
names <- rownames(na_count)
# nullify index
rownames(na_count) <- NULL
na_count <- cbind(names,na_count)

na_count = mutate(na_count, percentageNA = 100*na_count/dim(data.work)[1])
na_count = arrange(na_count, desc(percentageNA))
na_count
```

impute
- fin_situ
- province
- company_size
- monthly_pay

## TODO



// later we will need to deal with company size and the pay (being strings)







## CLeaning the extra data (from the personality attribute tests)
After some exploration, we realised only opt has NAs

Clean opt because it has NAs
```{r}
opt = na.omit(opt)
```

Grit has multiple readings per individual, visualising this, which plots a graph of number of people against how many grit readings they have

```{r}
grit_analysis  = group_by(grit, unid) %>% count() %>% group_by(n) %>% count()
ggplot(grit_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of grit readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(grit_analysis$n), max(grit_analysis$n), by =1),1))
```


```{r}
opt_analysis  = group_by(opt, unid) %>% count() %>% group_by(n) %>% count()
ggplot(opt_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of opt readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(opt_analysis$n), max(opt_analysis$n), by =1),1)) 
```

```{r}
com_analysis  = group_by(com, unid) %>% count() %>% group_by(n) %>% count()
ggplot(com_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of com readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(com_analysis$n), max(com_analysis$n), by =1),1))
```


```{r}
cft_analysis  = group_by(cft, unid) %>% count() %>% group_by(n) %>% count()
ggplot(cft_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of cft readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(cft_analysis$n), max(cft_analysis$n), by =1),1))
```

```{r}
num_analysis  = group_by(num, unid) %>% count() %>% group_by(n) %>% count()
ggplot(num_analysis, aes(x=n,y=nn)) + geom_bar(stat="identity") + xlab("Number of num readings") + ylab("Number of people") + scale_x_continuous(breaks = round(seq(min(num_analysis$n), max(num_analysis$n), by =1),1))
```

We will just take the mean to break ties

```{r}
grit_clean = group_by(grit,unid)  %>% summarise(mean_grit_score=mean(grit_score))
grit_clean$mean_grit_score = round(grit_clean$mean_grit_score)

com_clean = group_by(com,unid)  %>% summarise(mean_com_score=mean(com_score))
com_clean$mean_com_score = round(com_clean$mean_com_score)

opt_clean = group_by(opt,unid)  %>% summarise(mean_opt_score=mean(opt_score))
opt_clean$mean_opt_score = round(opt_clean$mean_opt_score)

cft_clean = group_by(cft,unid)  %>% summarise(mean_cft_score=mean(cft_score))
cft_clean$mean_cft_score = round(cft_clean$mean_cft_score)

num_clean = group_by(num,unid)  %>% summarise(mean_num_score=mean(num_score))
num_clean$mean_num_score = round(num_clean$mean_num_score)
```


merging the datasets
```{r}
data.work.merged = data.work %>% merge(opt_clean, by="unid", all.x=T) %>% merge(num_clean, by="unid", all.x=T) %>% merge(grit_clean,by="unid", all.x=T) %>% merge(cft_clean, by="unid", all.x=T) %>% merge(com_clean, by="unid", all.x=T)

summary(as.factor(data.work.merged$province))

```

remove the columns company_size and monthly_pay

```{r}
data.work.merged = select(data.work.merged, -company_size, -monthly_pay)
```

```{r}
na_count = sapply(data.work.merged, function(y) sum(length(which(is.na(y)))))
na_count = data.frame(na_count)
dim(na_count)

# create column from index
names <- rownames(na_count)
# nullify index
rownames(na_count) <- NULL
na_count <- cbind(names,na_count)

na_count = mutate(na_count, percentageNA = 100*na_count/dim(data.work.merged)[1])
na_count = arrange(na_count, desc(percentageNA))
na_count
```


clean the data of nas

```{r}
cols = c("province","volunteer", "leadershiprole", "anygrant", "gender", "moreThanSixMths")
data.work.merged[,cols] <- lapply(data.work.merged[,cols],as.factor)

data.work.rm.rows = select(na.omit(data.work.merged), -daysWorked, -survey_date_month, -survey_num, -unid)

data.work.rm.columns = select(data.work.merged, -mean_num_score, -province, -mean_com_score, -fin_situ_now, -fin_situ_future, -fin_situ_change, -mean_opt_score, -mean_cft_score, -daysWorked, -survey_date_month, -survey_num, -unid)
```


Finalising the dataset
```{r}
data.work.final = data.work.rm.columns
```

## performing logistic regression

```{r}
df = data.work.final
# split the data

set.seed(3456)
trainIndex <- createDataPartition(df$moreThanSixMths, p = .8, 
                                  list = FALSE, 
                                  times = 1)

dfTrain <- df[ trainIndex,]
dfTest  <- df[-trainIndex,]

summary(dfTrain)


TRAINCONTROL = trainControl(method = "cv", number = 5, verboseIter = TRUE)

model <- train(moreThanSixMths~., data = dfTrain, method = "svmLinearWeights", trControl = TRAINCONTROL)

?train
model
```

```{r}
# test model on training data
my_results <- predict(model, dfTest)

table(my_results, dfTest$moreThanSixMths)

# make confusion table
confusionMatrix(table(my_results, dfTest$moreThanSixMths))

summary(model)
```
















